"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
import joblib
import os
import warnings
warnings.filterwarnings('ignore')

def load_and_analyze_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
    print("üìä –ó–ê–ì–†–£–ó–ö–ê –ò –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•")
    print("=" * 50)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv('../../data/raw/wine_quality.csv')
    
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {df.isnull().sum().sum()}")
    print(f"–î—É–±–ª–∏–∫–∞—Ç—ã: {df.duplicated().sum()}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    print(f"\nüéØ –¶–ï–õ–ï–í–ê–Ø –ü–ï–†–ï–ú–ï–ù–ù–ê–Ø (quality)")
    print("=" * 30)
    print(df['quality'].value_counts().sort_index())
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {df['quality'].mean():.2f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {df['quality'].std():.2f}")
    
    return df

def advanced_preprocessing(df):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("\nüîß –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê")
    print("=" * 40)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö
    df_processed = df.copy()
    
    # 1. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    initial_size = len(df_processed)
    df_processed = df_processed.drop_duplicates()
    print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {initial_size - len(df_processed)}")
    
    # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é IQR
    numeric_columns = df_processed.select_dtypes(include=[np.number]).columns
    outliers_removed = 0
    
    for col in numeric_columns:
        if col != 'quality':  # –ù–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            Q1 = df_processed[col].quantile(0.25)
            Q3 = df_processed[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = ((df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)).sum()
            outliers_removed += outliers
            
            # –£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã
            df_processed = df_processed[(df_processed[col] >= lower_bound) & (df_processed[col] <= upper_bound)]
    
    print(f"–£–¥–∞–ª–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {outliers_removed}")
    print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {df_processed.shape}")
    
    # 3. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    
    # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
    df_processed['acid_ratio'] = df_processed['fixed.acidity'] / (df_processed['volatile.acidity'] + 1e-8)
    df_processed['sulfur_ratio'] = df_processed['free.sulfur.dioxide'] / (df_processed['total.sulfur.dioxide'] + 1e-8)
    df_processed['alcohol_density_ratio'] = df_processed['alcohol'] / df_processed['density']
    
    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
    df_processed['log_residual_sugar'] = np.log1p(df_processed['residual.sugar'])
    df_processed['log_chlorides'] = np.log1p(df_processed['chlorides'])
    
    # –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df_processed['alcohol_squared'] = df_processed['alcohol'] ** 2
    df_processed['ph_squared'] = df_processed['pH'] ** 2
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∞–ª–∫–æ–≥–æ–ª—è
    df_processed['alcohol_category'] = pd.cut(df_processed['alcohol'], 
                                            bins=[0, 10, 12, 15], 
                                            labels=['low', 'medium', 'high'])
    
    # One-hot encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    df_processed = pd.get_dummies(df_processed, columns=['alcohol_category'], prefix='alcohol')
    
    print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ feature engineering: {df_processed.shape}")
    
    return df_processed

def feature_selection(X, y):
    """–û—Ç–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\nüéØ –û–¢–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í")
    print("=" * 25)
    
    # 1. Univariate feature selection
    selector_univariate = SelectKBest(score_func=f_classif, k=15)
    X_selected = selector_univariate.fit_transform(X, y)
    selected_features = X.columns[selector_univariate.get_support()]
    print(f"–û—Ç–æ–±—Ä–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (univariate): {len(selected_features)}")
    print(f"–ü—Ä–∏–∑–Ω–∞–∫–∏: {list(selected_features)}")
    
    return X_selected, selected_features

def train_advanced_models(X_train, X_test, y_train, y_test):
    """–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    print("\nü§ñ –û–ë–£–ß–ï–ù–ò–ï –ü–†–û–î–í–ò–ù–£–¢–´–• –ú–û–î–ï–õ–ï–ô")
    print("=" * 45)
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    models = {}
    
    # 1. Random Forest —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    print("1. Random Forest...")
    rf_params = {
        'n_estimators': [200, 300, 500],
        'max_depth': [15, 20, 25, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2']
    }
    
    rf = RandomForestClassifier(random_state=42, n_jobs=-1)
    rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=0)
    rf_grid.fit(X_train, y_train)
    models['RandomForest'] = rf_grid.best_estimator_
    print(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {rf_grid.best_params_}")
    print(f"   –õ—É—á—à–∏–π score: {rf_grid.best_score_:.4f}")
    
    # 2. Gradient Boosting
    print("2. Gradient Boosting...")
    gb_params = {
        'n_estimators': [200, 300],
        'learning_rate': [0.05, 0.1, 0.15],
        'max_depth': [6, 8, 10],
        'subsample': [0.8, 0.9, 1.0]
    }
    
    gb = GradientBoostingClassifier(random_state=42)
    gb_grid = GridSearchCV(gb, gb_params, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=0)
    gb_grid.fit(X_train, y_train)
    models['GradientBoosting'] = gb_grid.best_estimator_
    print(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {gb_grid.best_params_}")
    print(f"   –õ—É—á—à–∏–π score: {gb_grid.best_score_:.4f}")
    
    # 3. Extra Trees
    print("3. Extra Trees...")
    et_params = {
        'n_estimators': [200, 300, 500],
        'max_depth': [15, 20, 25, None],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    }
    
    et = ExtraTreesClassifier(random_state=42, n_jobs=-1)
    et_grid = GridSearchCV(et, et_params, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=0)
    et_grid.fit(X_train, y_train)
    models['ExtraTrees'] = et_grid.best_estimator_
    print(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {et_grid.best_params_}")
    print(f"   –õ—É—á—à–∏–π score: {et_grid.best_score_:.4f}")
    
    # 4. SVM —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    print("4. SVM...")
    svm_params = {
        'C': [1, 10, 100],
        'gamma': ['scale', 'auto', 0.001, 0.01],
        'kernel': ['rbf', 'poly']
    }
    
    svm = SVC(random_state=42, probability=True)
    svm_grid = GridSearchCV(svm, svm_params, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=0)
    svm_grid.fit(X_train_scaled, y_train)
    models['SVM'] = svm_grid.best_estimator_
    print(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {svm_grid.best_params_}")
    print(f"   –õ—É—á—à–∏–π score: {svm_grid.best_score_:.4f}")
    
    # 5. Ensemble (Voting Classifier)
    print("5. Ensemble (Voting)...")
    voting_clf = VotingClassifier(
        estimators=[
            ('rf', models['RandomForest']),
            ('gb', models['GradientBoosting']),
            ('et', models['ExtraTrees']),
            ('svm', models['SVM'])
        ],
        voting='soft'
    )
    voting_clf.fit(X_train, y_train)
    models['Ensemble'] = voting_clf
    
    return models, scaler

def evaluate_models(models, X_test, y_test, scaler):
    """–û—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    print("\nüìä –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ï–ô")
    print("=" * 30)
    
    results = {}
    
    for name, model in models.items():
        print(f"\n{name}:")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        if name == 'SVM':
            X_test_scaled = scaler.transform(X_test)
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)
        else:
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        results[name] = {
            'accuracy': accuracy,
            'f1_score': f1,
            'predictions': y_pred,
            'probabilities': y_pred_proba
        }
        
        print(f"   Accuracy: {accuracy:.4f}")
        print(f"   F1-score: {f1:.4f}")
        
        # Cross-validation
        if name == 'SVM':
            cv_scores = cross_val_score(model, scaler.transform(X_test), y_test, cv=5, scoring='f1_weighted')
        else:
            cv_scores = cross_val_score(model, X_test, y_test, cv=5, scoring='f1_weighted')
        
        print(f"   CV F1-score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    return results

def save_best_model(models, results, scaler, selected_features):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å"""
    print("\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò")
    print("=" * 35)
    
    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ F1-score
    best_model_name = max(results.keys(), key=lambda x: results[x]['f1_score'])
    best_model = models[best_model_name]
    best_score = results[best_model_name]['f1_score']
    
    print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
    print(f"F1-score: {best_score:.4f}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    os.makedirs('../../models', exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_path = '../../models/wine_quality_advanced_model.pkl'
    joblib.dump(best_model, model_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
    scaler_path = '../../models/wine_scaler.pkl'
    joblib.dump(scaler, scaler_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    features_path = '../../models/selected_features.pkl'
    joblib.dump(selected_features, features_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics_path = '../../models/advanced_model_metrics.txt'
    with open(metrics_path, 'w') as f:
        f.write(f"Advanced Wine Quality Model\n")
        f.write(f"Best Model: {best_model_name}\n")
        f.write(f"F1-score: {best_score:.4f}\n")
        f.write(f"Accuracy: {results[best_model_name]['accuracy']:.4f}\n")
        f.write(f"Selected Features: {len(selected_features)}\n")
        f.write(f"Features: {list(selected_features)}\n")
    
    print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    print(f"Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {scaler_path}")
    print(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {features_path}")
    print(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {metrics_path}")
    
    return best_model_name, best_model

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üç∑ –ü–†–û–î–í–ò–ù–£–¢–û–ï –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ö–ê–ß–ï–°–¢–í–ê –í–ò–ù–ê")
    print("=" * 60)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    df = load_and_analyze_data()
    
    # 2. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    df_processed = advanced_preprocessing(df)
    
    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X = df_processed.drop('quality', axis=1)
    y = df_processed['quality']
    
    # 4. –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_selected, selected_features = feature_selection(X, y)
    
    # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X_selected, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nüìä –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•")
    print("=" * 25)
    print(f"Train set: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"Test set: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train.shape[1]}")
    
    # 6. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    models, scaler = train_advanced_models(X_train, X_test, y_train, y_test)
    
    # 7. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
    results = evaluate_models(models, X_test, y_test, scaler)
    
    # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    best_model_name, best_model = save_best_model(models, results, scaler, selected_features)
    
    print(f"\nüéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 30)
    print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
    print(f"F1-score: {results[best_model_name]['f1_score']:.4f}")
    print(f"Accuracy: {results[best_model_name]['accuracy']:.4f}")

if __name__ == "__main__":
    main()

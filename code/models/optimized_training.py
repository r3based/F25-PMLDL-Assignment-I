"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif, RFE
import xgboost as xgb
import lightgbm as lgb
import joblib
import os
import warnings
import time
warnings.filterwarnings('ignore')

def load_and_analyze_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
    print("üìä –ó–ê–ì–†–£–ó–ö–ê –ò –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•")
    print("=" * 50)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv('../../data/raw/wine_quality.csv')
    
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {df.isnull().sum().sum()}")
    print(f"–î—É–±–ª–∏–∫–∞—Ç—ã: {df.duplicated().sum()}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    print(f"\nüéØ –¶–ï–õ–ï–í–ê–Ø –ü–ï–†–ï–ú–ï–ù–ù–ê–Ø (quality)")
    print("=" * 30)
    print(df['quality'].value_counts().sort_index())
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {df['quality'].mean():.2f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {df['quality'].std():.2f}")
    
    return df

def advanced_preprocessing(df):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("\nüîß –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê")
    print("=" * 40)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö
    df_processed = df.copy()
    
    # 1. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    initial_size = len(df_processed)
    df_processed = df_processed.drop_duplicates()
    print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {initial_size - len(df_processed)}")
    
    # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é IQR
    numeric_columns = df_processed.select_dtypes(include=[np.number]).columns
    outliers_removed = 0
    
    for col in numeric_columns:
        if col != 'quality':  # –ù–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            Q1 = df_processed[col].quantile(0.25)
            Q3 = df_processed[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = ((df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)).sum()
            outliers_removed += outliers
            
            # –£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã
            df_processed = df_processed[(df_processed[col] >= lower_bound) & (df_processed[col] <= upper_bound)]
    
    print(f"–£–¥–∞–ª–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {outliers_removed}")
    print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {df_processed.shape}")
    
    # 3. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    
    # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
    df_processed['acid_ratio'] = df_processed['fixed.acidity'] / (df_processed['volatile.acidity'] + 1e-8)
    df_processed['sulfur_ratio'] = df_processed['free.sulfur.dioxide'] / (df_processed['total.sulfur.dioxide'] + 1e-8)
    df_processed['alcohol_density_ratio'] = df_processed['alcohol'] / df_processed['density']
    
    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
    df_processed['log_residual_sugar'] = np.log1p(df_processed['residual.sugar'])
    df_processed['log_chlorides'] = np.log1p(df_processed['chlorides'])
    
    # –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df_processed['alcohol_squared'] = df_processed['alcohol'] ** 2
    df_processed['ph_squared'] = df_processed['pH'] ** 2
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∞–ª–∫–æ–≥–æ–ª—è
    df_processed['alcohol_category'] = pd.cut(df_processed['alcohol'], 
                                            bins=[0, 10, 12, 15], 
                                            labels=['low', 'medium', 'high'])
    
    # One-hot encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    df_processed = pd.get_dummies(df_processed, columns=['alcohol_category'], prefix='alcohol')
    
    print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ feature engineering: {df_processed.shape}")
    
    return df_processed

def feature_selection(X, y):
    """–û—Ç–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\nüéØ –û–¢–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í")
    print("=" * 25)
    
    # 1. Univariate feature selection
    selector_univariate = SelectKBest(score_func=f_classif, k=20)
    X_selected = selector_univariate.fit_transform(X, y)
    selected_features = X.columns[selector_univariate.get_support()]
    print(f"–û—Ç–æ–±—Ä–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (univariate): {len(selected_features)}")
    print(f"–ü—Ä–∏–∑–Ω–∞–∫–∏: {list(selected_features)}")
    
    return X_selected, selected_features

def train_optimized_models(X_train, X_test, y_train, y_test):
    """–û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    print("\nüöÄ –û–ë–£–ß–ï–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–• –ú–û–î–ï–õ–ï–ô")
    print("=" * 50)
    
    models = {}
    training_times = {}
    
    # 1. XGBoost (CPU –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
    print("1. XGBoost (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)...")
    start_time = time.time()
    
    # –ö–æ–¥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è XGBoost
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    y_test_encoded = le.transform(y_test)
    
    xgb_params = {
        'n_estimators': [500, 1000],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.05, 0.1],
        'subsample': [0.8, 0.9],
        'colsample_bytree': [0.8, 0.9],
        'random_state': [42]
    }
    
    xgb_model = xgb.XGBClassifier(
        random_state=42,
        eval_metric='mlogloss',
        n_jobs=-1
    )
    
    xgb_grid = GridSearchCV(
        xgb_model, 
        xgb_params, 
        cv=5, 
        scoring='f1_weighted', 
        n_jobs=-1,
        verbose=1
    )
    
    xgb_grid.fit(X_train, y_train_encoded)
    models['XGBoost'] = xgb_grid.best_estimator_
    training_times['XGBoost'] = time.time() - start_time
    
    print(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {xgb_grid.best_params_}")
    print(f"   –õ—É—á—à–∏–π score: {xgb_grid.best_score_:.4f}")
    print(f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_times['XGBoost']:.2f} —Å–µ–∫")
    
    # 2. LightGBM
    print("2. LightGBM...")
    start_time = time.time()
    
    lgb_params = {
        'n_estimators': [500, 1000],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.05, 0.1],
        'subsample': [0.8, 0.9],
        'colsample_bytree': [0.8, 0.9],
        'random_state': [42]
    }
    
    lgb_model = lgb.LGBMClassifier(
        random_state=42,
        verbose=-1,
        n_jobs=-1
    )
    
    lgb_grid = GridSearchCV(
        lgb_model, 
        lgb_params, 
        cv=5, 
        scoring='f1_weighted', 
        n_jobs=-1,
        verbose=1
    )
    
    lgb_grid.fit(X_train, y_train)
    models['LightGBM'] = lgb_grid.best_estimator_
    training_times['LightGBM'] = time.time() - start_time
    
    print(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {lgb_grid.best_params_}")
    print(f"   –õ—É—á—à–∏–π score: {lgb_grid.best_score_:.4f}")
    print(f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_times['LightGBM']:.2f} —Å–µ–∫")
    
    # 3. Random Forest (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
    print("3. Random Forest (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)...")
    start_time = time.time()
    
    rf_params = {
        'n_estimators': [300, 500, 1000],
        'max_depth': [15, 20, 25, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2'],
        'bootstrap': [True, False]
    }
    
    rf = RandomForestClassifier(random_state=42, n_jobs=-1)
    rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1)
    rf_grid.fit(X_train, y_train)
    models['RandomForest'] = rf_grid.best_estimator_
    training_times['RandomForest'] = time.time() - start_time
    
    print(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {rf_grid.best_params_}")
    print(f"   –õ—É—á—à–∏–π score: {rf_grid.best_score_:.4f}")
    print(f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_times['RandomForest']:.2f} —Å–µ–∫")
    
    # 4. Gradient Boosting
    print("4. Gradient Boosting...")
    start_time = time.time()
    
    gb_params = {
        'n_estimators': [200, 300, 500],
        'learning_rate': [0.05, 0.1, 0.15],
        'max_depth': [6, 8, 10],
        'subsample': [0.8, 0.9, 1.0]
    }
    
    gb = GradientBoostingClassifier(random_state=42)
    gb_grid = GridSearchCV(gb, gb_params, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1)
    gb_grid.fit(X_train, y_train)
    models['GradientBoosting'] = gb_grid.best_estimator_
    training_times['GradientBoosting'] = time.time() - start_time
    
    print(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {gb_grid.best_params_}")
    print(f"   –õ—É—á—à–∏–π score: {gb_grid.best_score_:.4f}")
    print(f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_times['GradientBoosting']:.2f} —Å–µ–∫")
    
    # 5. Extra Trees
    print("5. Extra Trees...")
    start_time = time.time()
    
    et_params = {
        'n_estimators': [300, 500, 1000],
        'max_depth': [15, 20, 25, None],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    }
    
    et = ExtraTreesClassifier(random_state=42, n_jobs=-1)
    et_grid = GridSearchCV(et, et_params, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1)
    et_grid.fit(X_train, y_train)
    models['ExtraTrees'] = et_grid.best_estimator_
    training_times['ExtraTrees'] = time.time() - start_time
    
    print(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {et_grid.best_params_}")
    print(f"   –õ—É—á—à–∏–π score: {et_grid.best_score_:.4f}")
    print(f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_times['ExtraTrees']:.2f} —Å–µ–∫")
    
    # 6. Ensemble (Voting Classifier)
    print("6. Ensemble (Voting)...")
    start_time = time.time()
    
    voting_clf = VotingClassifier(
        estimators=[
            ('xgb', models['XGBoost']),
            ('lgb', models['LightGBM']),
            ('rf', models['RandomForest']),
            ('gb', models['GradientBoosting']),
            ('et', models['ExtraTrees'])
        ],
        voting='soft'
    )
    voting_clf.fit(X_train, y_train)
    models['Ensemble'] = voting_clf
    training_times['Ensemble'] = time.time() - start_time
    
    print(f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_times['Ensemble']:.2f} —Å–µ–∫")
    
    return models, training_times, le

def evaluate_models(models, X_test, y_test, training_times, le):
    """–û—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    print("\nüìä –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ï–ô")
    print("=" * 30)
    
    results = {}
    
    for name, model in models.items():
        print(f"\n{name}:")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        start_time = time.time()
        if name == 'XGBoost':
            y_pred_encoded = model.predict(X_test)
            y_pred = le.inverse_transform(y_pred_encoded)
            y_pred_proba = model.predict_proba(X_test)
        else:
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)
        
        prediction_time = time.time() - start_time
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        results[name] = {
            'accuracy': accuracy,
            'f1_score': f1,
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'training_time': training_times[name],
            'prediction_time': prediction_time
        }
        
        print(f"   Accuracy: {accuracy:.4f}")
        print(f"   F1-score: {f1:.4f}")
        print(f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_times[name]:.2f} —Å–µ–∫")
        print(f"   –í—Ä–µ–º—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {prediction_time:.4f} —Å–µ–∫")
        
        # Cross-validation
        if name == 'XGBoost':
            cv_scores = cross_val_score(model, X_test, le.transform(y_test), cv=5, scoring='f1_weighted')
        else:
            cv_scores = cross_val_score(model, X_test, y_test, cv=5, scoring='f1_weighted')
        
        print(f"   CV F1-score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    return results

def save_best_model(models, results, selected_features, le):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å"""
    print("\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò")
    print("=" * 35)
    
    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ F1-score
    best_model_name = max(results.keys(), key=lambda x: results[x]['f1_score'])
    best_model = models[best_model_name]
    best_score = results[best_model_name]['f1_score']
    
    print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
    print(f"F1-score: {best_score:.4f}")
    print(f"Accuracy: {results[best_model_name]['accuracy']:.4f}")
    print(f"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {results[best_model_name]['training_time']:.2f} —Å–µ–∫")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    os.makedirs('../../models', exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_path = '../../models/wine_quality_optimized_model.pkl'
    joblib.dump(best_model, model_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º label encoder –¥–ª—è XGBoost
    if best_model_name == 'XGBoost':
        le_path = '../../models/label_encoder.pkl'
        joblib.dump(le, le_path)
        print(f"Label encoder —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {le_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    features_path = '../../models/optimized_selected_features.pkl'
    joblib.dump(selected_features, features_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics_path = '../../models/optimized_model_metrics.txt'
    with open(metrics_path, 'w') as f:
        f.write(f"Optimized Wine Quality Model\n")
        f.write(f"Best Model: {best_model_name}\n")
        f.write(f"F1-score: {best_score:.4f}\n")
        f.write(f"Accuracy: {results[best_model_name]['accuracy']:.4f}\n")
        f.write(f"Training Time: {results[best_model_name]['training_time']:.2f} —Å–µ–∫\n")
        f.write(f"Prediction Time: {results[best_model_name]['prediction_time']:.4f} —Å–µ–∫\n")
        f.write(f"Selected Features: {len(selected_features)}\n")
        f.write(f"Features: {list(selected_features)}\n")
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        f.write(f"\n–í—Å–µ –º–æ–¥–µ–ª–∏:\n")
        for name, result in results.items():
            f.write(f"{name}: F1={result['f1_score']:.4f}, Acc={result['accuracy']:.4f}, Time={result['training_time']:.2f}s\n")
    
    print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    print(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {features_path}")
    print(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {metrics_path}")
    
    return best_model_name, best_model

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ö–ê–ß–ï–°–¢–í–ê –í–ò–ù–ê")
    print("=" * 60)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    df = load_and_analyze_data()
    
    # 2. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    df_processed = advanced_preprocessing(df)
    
    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X = df_processed.drop('quality', axis=1)
    y = df_processed['quality']
    
    # 4. –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_selected, selected_features = feature_selection(X, y)
    
    # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X_selected, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nüìä –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•")
    print("=" * 25)
    print(f"Train set: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"Test set: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train.shape[1]}")
    
    # 6. –û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    models, training_times, le = train_optimized_models(X_train, X_test, y_train, y_test)
    
    # 7. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
    results = evaluate_models(models, X_test, y_test, training_times, le)
    
    # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    best_model_name, best_model = save_best_model(models, results, selected_features, le)
    
    print(f"\nüéâ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 50)
    print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
    print(f"F1-score: {results[best_model_name]['f1_score']:.4f}")
    print(f"Accuracy: {results[best_model_name]['accuracy']:.4f}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
    baseline_accuracy = 0.6454  # –ò–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª–∏
    improvement = (results[best_model_name]['accuracy'] - baseline_accuracy) / baseline_accuracy * 100
    print(f"–£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏: {improvement:.1f}%")

if __name__ == "__main__":
    main()

